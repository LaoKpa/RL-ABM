{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "#import math\n",
    "#import matplotlib.pyplot as plt\n",
    "#from initialize_model import *\n",
    "from functions.helpers import organise_data, hypothetical_series, get_specific_bootstraps_moments, confidence_interval\n",
    "#from functions.inequality import gini\n",
    "#from model import *\n",
    "#import statsmodels.api as sm\n",
    "from functions.stylizedfacts import autocorrelation_returns\n",
    "#from matplotlib import style\n",
    "from functions.indirect_calibration import quadratic_loss_function\n",
    "import scipy.stats as stats\n",
    "from SALib.sample import latin\n",
    "from hurst import compute_Hc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asset price volatility and wealth inequality\n",
    "\n",
    "This notebook contains the following steps:\n",
    "\n",
    "1. Parameter calibration and estimation\n",
    "2. Model dynamics\n",
    "3. Experiment\n",
    "\n",
    "## 1 Parameter calibration and estimation\n",
    "\n",
    "### 1.1 Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "shiller_data = pd.read_excel('http://www.econ.yale.edu/~shiller/data/ie_data.xls', header=7)[:-3]\n",
    "p = pd.Series(np.array(shiller_data.iloc[1174:-1]['Price']))\n",
    "price_div = pd.Series(np.array(shiller_data.iloc[1174:-1]['CAPE']))\n",
    "p_returns = pd.Series(np.array(shiller_data.iloc[1174:]['Price'])).pct_change()[1:]\n",
    "pd_returns = pd.Series(np.array(shiller_data.iloc[1174:]['CAPE'])).pct_change()[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 calibration\n",
    "\n",
    "First, I set two parameters for computational efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I calibrate parameters using data and literature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"trader_sample_size\": 10, # selected for comp efficiency\n",
    "          \"n_traders\": 1000, # selected for comp efficiency\n",
    "          \"init_stocks\": int((21780000000 / 267.33) / float(1000000)), # market valuation of Vanguard S&P 500 / share price \n",
    "          \"ticks\": len(p), # lenght of reference data\n",
    "          \"fundamental_value\": p.mean(), # average value of reference data, assuming efficient markets\n",
    "          \"std_fundamental\": p_returns.std(), # standard deviation of returns sp price, assuming efficient markets\n",
    "          \"base_risk_aversion\": 0.7, # estimate from Kim & Lee (2012)\n",
    "          'spread_max': 0.004087, # estimate from Riordan & Storkenmaier (2012)\n",
    "          \"horizon\": int(len(p) * 0.35), # estimate based on average churn ratio found by Cella, Ellul and Giannetti (2013)\n",
    "          # estimated parameters\n",
    "          \"std_noise\": 0.01, \n",
    "          \"w_random\": 1.0, \n",
    "          \"strat_share_chartists\": 0.0,\n",
    "          # parameter only used for experiment\n",
    "          \"mean_reversion\": 0.0,\n",
    "          # fixed / not modelled parameters\n",
    "          \"fundamentalist_horizon_multiplier\": 1.0,\n",
    "          \"mutation_intensity\": 0.0,\n",
    "          \"average_learning_ability\": 0.0,\n",
    "          \"trades_per_tick\": 1\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, there are seven parameters left which are difficult to calibrate. Therefore, I estimate these values using the method of simulated moments. The starting point of this method is finding appropriate moments which the model should be able to replicate. I note that there should be more moments than parameters. Since 3 parameters need to be estimated, 4 moments are needed. Since this is a highly stylized model, I am only interested in the model replicating some basic moments of the price return series. The moments are the autocorrelation, autocorrelation of absolute returns, kurtosis of returns and hurst of the price. \n",
    "\n",
    "First, I calculate these moments for the empirical data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_moments = np.array([\n",
    "    autocorrelation_returns(p_returns, 25),\n",
    "    autocorrelation_returns(p_returns.abs(), 25),\n",
    "    p_returns.kurtosis(),\n",
    "    compute_Hc(p, kind='price', simplified=True)[0]\n",
    "    ])\n",
    "emp_moments\n",
    "np.save('emp_moments', emp_moments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To account for the fact that some of these moments might correlate over different Monte Carlo Simulations, the MSM seeks to obtain a variance covariance matrix of the moments. Since there is only one empirical reality, I use a bootstrap procedure to create a covariance matrix of empirical moments. For this, I use a block bootstrap procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE = 25\n",
    "BOOTSTRAPS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_data_blocks = []\n",
    "price_data_blocks = []\n",
    "for x in range(0, len(p_returns[:-3]), BLOCK_SIZE):\n",
    "    p_data_blocks.append(p_returns[x:x + BLOCK_SIZE])\n",
    "    price_data_blocks.append(p[x:x + BLOCK_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrapped_p_returns = []\n",
    "bootstrapped_prices = []\n",
    "for i in range(BOOTSTRAPS):\n",
    "    sim_data_p = [random.choice(p_data_blocks) for _ in p_data_blocks]\n",
    "    sim_data2_p = [j for i in sim_data_p for j in i]\n",
    "    bootstrapped_p_returns.append(sim_data2_p)\n",
    "    \n",
    "    sim_data_price = [random.choice(price_data_blocks) for _ in price_data_blocks]\n",
    "    sim_data2_price = [j for i in sim_data_price for j in i]\n",
    "    bootstrapped_prices.append(sim_data2_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets_autocor = []\n",
    "rets_abs_autocors = []\n",
    "kurts = []\n",
    "hursts = []\n",
    "\n",
    "for rets, prices in list(zip(bootstrapped_p_returns, bootstrapped_prices)):\n",
    "    rets_autocor.append(autocorrelation_returns(rets, 25))\n",
    "    rets_abs_autocors.append(autocorrelation_returns(np.abs(rets), 25))\n",
    "    kurts.append(pd.Series(rets).kurtosis())\n",
    "    hursts.append(compute_Hc(prices, kind='price', simplified=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bootstrapped_moments = [\n",
    "                            rets_autocor,\n",
    "                            rets_abs_autocors,\n",
    "                            kurts,\n",
    "                            hursts\n",
    "                           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "av_moments = [np.nanmean(x) for x in all_bootstrapped_moments]\n",
    "moments_b = [get_specific_bootstraps_moments(all_bootstrapped_moments, n) for n in range(len(bootstrapped_p_returns))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I follow [Franke & Westerhoff 2016](https://link.springer.com/article/10.1007/s11403-014-0140-6#Sec8) in that I use the inverse of the bootstrap estimate of the moment covariance matrix as my weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_hat = 1.0 / len(bootstrapped_p_returns) * sum([np.dot(np.array([(mb - av_moments)]).transpose(), np.array([(mb - av_moments)])) for mb in moments_b])\n",
    "W = np.linalg.inv(W_hat)\n",
    "np.save('distr_weighting_matrix', W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I establish confidence intervals for the moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_intervals = [confidence_interval(m, emp) for m, emp in zip(all_bootstrapped_moments, emp_moments)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_values = []\n",
    "for b in moments_b:\n",
    "    j_values.append(quadratic_loss_function(b, emp_moments, W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [0 for x in moments_b[0]]\n",
    "for bootstr in range(len(moments_b)):\n",
    "    for idx, moment in enumerate(moments_b[bootstr]):\n",
    "        if moment > confidence_intervals[idx][0] and moment < confidence_intervals[idx][1]:\n",
    "            scores[idx] += 1\n",
    "MCR_bootstrapped_moments = np.array(scores) / (np.ones(len(scores)) * len(moments_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the estimation procedure, I first sample the parameter space using Latin Hypercube sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = {\n",
    "  'num_vars': 3,\n",
    "  'names': ['std_noise', \"w_random\", \"strat_share_chartists\"],\n",
    "  'bounds': [[0.03, 0.09], [0.02, 0.15], [0.02, 0.7]]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.05704321942550724, 0.11590615914660493, 0.502318253496888]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latin_hyper_cube = latin.sample(problem=problem, N=population_size)\n",
    "latin_hyper_cube = latin_hyper_cube.tolist()\n",
    "with open('hypercube.txt', 'w') as f:\n",
    "    json.dump(latin_hyper_cube, f)\n",
    "initial_params = latin_hyper_cube[0]\n",
    "initial_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I perform the estimation excercise in a different Python file using multi-processing. These will serve as the starting parameters to do simulation. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
